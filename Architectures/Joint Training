## Joint training architecture using early fusion strategy.

import os
import torch
import torch.nn as nn
import torch.optim as optim
import timm
import torchaudio.transforms as transforms
import torch.nn.functional as F
import matplotlib.pyplot as plt

# 1. Define Joint Transformer Model
class JointDeepfakeDetectionModel(nn.Module):
    def __init__(self, num_classes=2):
        super(JointDeepfakeDetectionModel, self).__init__()
        # Initialize video and audio Vision Transformer models
        self.video_model = timm.create_model('vit_tiny_patch16_224', pretrained=True)
        self.audio_model = timm.create_model('vit_tiny_patch16_224', pretrained=True)

        # Get the feature sizes from both models
        self.video_feature_size = self.video_model.embed_dim
        self.audio_feature_size = self.audio_model.embed_dim

        # Print the feature sizes for debugging
        print(f"Video feature size: {self.video_feature_size}")
        print(f"Audio feature size: {self.audio_feature_size}")

        # The classifier will take in the combined feature sizes from both models
        combined_feature_size = self.video_feature_size + self.audio_feature_size
        self.classifier = nn.Linear(combined_feature_size, num_classes)  

    def forward(self, video_data, audio_data):
        # Extract video and audio features
        video_features = self.video_model.forward_features(video_data)  
        audio_features = self.audio_model.forward_features(audio_data)  

        # Debug: Print the feature shapes for video and audio
        print(f"Video features shape: {video_features.shape}")
        print(f"Audio features shape: {audio_features.shape}")

        # Use the CLS token (the first token) from both video and audio features
        video_cls_token = video_features[:, 0, :]  
        audio_cls_token = audio_features[:, 0, :] 

        # Debug: Print the CLS token shapes
        print(f"Video CLS token shape: {video_cls_token.shape}")
        print(f"Audio CLS token shape: {audio_cls_token.shape}")

        # Concatenate the CLS tokens from video and audio
        combined_features = torch.cat([video_cls_token, audio_cls_token], dim=1) 

        # Pass the combined features through the classifier
        return self.classifier(combined_features)

# 2. Initialize the model, loss function, optimizer, and GradScaler
model = JointDeepfakeDetectionModel(num_classes=2)
criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(model.parameters(), lr=0.001)
scaler = torch.amp.GradScaler()

# Move model to GPU
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model.to(device)

# 3. Audio transformation to convert raw audio to 2D mel-spectrogram
mel_spectrogram_transform = transforms.MelSpectrogram(
    sample_rate=44100,
    n_fft=1024,
    hop_length=512,
    n_mels=128
)

# 4. Training Loop with Gradient Accumulation, Mixed Precision, and Checkpoint Saving
num_epochs = 100
accumulation_steps = 4
checkpoint_dir = '/content/drive/MyDrive/Jointtraining_checkpoints'
os.makedirs(checkpoint_dir, exist_ok=True)

losses = []

for epoch in range(num_epochs):
    model.train()
    optimizer.zero_grad()
    running_loss = 0.0

    for i, ((video_data, video_labels), (audio_data, audio_labels)) in enumerate(zip(train_loader, audio_loader)):
        video_data, audio_data = video_data.to(device), audio_data.to(device)

        # Convert the raw audio data to mel-spectrogram
        audio_data_cpu = audio_data.cpu()
        audio_data_mel = mel_spectrogram_transform(audio_data_cpu)

        # Move mel-spectrogram back to GPU
        audio_data = audio_data_mel.to(device)

        # Ensure the audio data has exactly 4 dimensions: [B, 1, n_mels, time_steps]
        if audio_data.dim() == 3:
            audio_data = audio_data.unsqueeze(1)  

        # Resize the audio data to [B, 1, 224, 224] to match Vision Transformer input
        audio_data = F.interpolate(audio_data, size=(224, 224), mode='bilinear', align_corners=False)

        # Convert the 1-channel audio data to 3 channels
        audio_data = audio_data.repeat(1, 3, 1, 1) 

        # Debug: Check the shape of video_data and audio_data
        print(f"Batch {i + 1} - Video Data Shape: {video_data.shape}, Audio Data Shape: {audio_data.shape}")

        labels = video_labels.to(device)

        # Mixed precision forward pass
        with torch.amp.autocast(device_type='cuda'):
            outputs = model(video_data, audio_data)
            loss = criterion(outputs, labels)

            # Accumulate gradients
            scaler.scale(loss).backward()

            if (i + 1) % accumulation_steps == 0:
                scaler.step(optimizer)
                scaler.update()
                optimizer.zero_grad()

            running_loss += loss.item()

        # Print and store loss every 10 batches
        if i % 10 == 9:
            print(f"[{epoch + 1}, {i + 1}] loss: {running_loss / 10:.3f}")
            losses.append(running_loss / 10)
            running_loss = 0.0

    # Save checkpoint every few epochs
    if (epoch + 1) % 10 == 0:
        checkpoint_path = os.path.join(checkpoint_dir, f"checkpoint_epoch_{epoch + 1}.pth")
        torch.save(model.state_dict(), checkpoint_path)
        print(f"Checkpoint saved: {checkpoint_path}")

# 5. Plot training loss
if losses:
    plt.plot(losses)
    plt.title('Training Loss over Batches')
    plt.xlabel('Batch')
    plt.ylabel('Loss')
    plt.show()
else:
    print("No losses to plot.")
